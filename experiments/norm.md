## AdamW + LayerNorm - baseline
iter 5000: loss 0.2551, time 19778.40ms, mfu 28.95%
train cost:1022.6063044071198s

## AdamW + Pre-RMSNorm
iter 5000: loss 0.2649, time 18861.17ms, mfu 29.23%
train cost:1009.4984974861145s



## Llama
step 1250: train loss 0.0584, val loss 4.2583
